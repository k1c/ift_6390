{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHXlGzGyLKLq",
    "colab_type": "text"
   },
   "source": [
    "### Question 4\n",
    "The method SVM.fit uses the code you wrote above to train the SVM. After each epoch (one pass through the training set), SVM.fit computes the training loss, the training accuracy, the test loss, and the test accuracy.\n",
    "    \n",
    "Plot the value of these four quantities for every epoch for C = 0.1, 1, 30, 50. \n",
    "\n",
    "Use 200 epochs, a learning rate of 0.001, and a minibatch size of 5000.\n",
    "    \n",
    "You should have four plots: one for each quantity, with the curves for all four values of C. \n",
    "Include these four plots in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-7d3SxaLLKLs",
    "colab_type": "code",
    "outputId": "5eb57089-609f-4e85-cfe4-58351e404b9c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.571150732046E12,
     "user_tz": 240.0,
     "elapsed": 4125564.0,
     "user": {
      "displayName": "Carolyne Pelletier",
      "photoUrl": "https://lh3.googleusercontent.com/-KxxYWjwuy4U/AAAAAAAAAAI/AAAAAAAAhlA/bx_WjqK5ddo/s64/photo.jpg",
      "userId": "15164509602820073060"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive/ift_6390/homework2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self,eta, C, niter, batch_size, verbose):\n",
    "        self.eta = eta; self.C = C; self.niter = niter; self.batch_size = batch_size; self.verbose = verbose\n",
    "\n",
    "#TODO check edges cases for m (can we assume that the max label number is smaller than m?)\n",
    "    def make_one_versus_all_labels(self, y, m):\n",
    "        \"\"\"\n",
    "\ty : numpy array of shape (n,)\n",
    "\tm : int (in this homework, m will be 10)\n",
    "\treturns : numpy array of shape (n,m)\n",
    "\t\"\"\"\n",
    "        ova = np.full((y.shape[0], m), -1)\n",
    "        for i, row in enumerate(ova):\n",
    "            row[y[i]] = 1\n",
    "        return ova\n",
    "\n",
    "    def compute_loss(self, x, y):\n",
    "        \"\"\"underunderunder\n",
    "\tx : numpy array of shape (minibatch size, 401)\n",
    "\ty : numpy array of shape (minibatch size, 10)\n",
    "\treturns : float\n",
    "\t\"\"\"\n",
    "        scores = x.dot(self.w)\n",
    "        margins = np.maximum(0, 1 - np.multiply(scores, y))\n",
    "        loss = np.mean(np.sum(margins, axis=1))\n",
    "        loss = self.C * loss\n",
    "        # Computes de regularization term\n",
    "        loss += 0.5 * np.sum(np.linalg.norm(self.w, ord=2, axis=0))\n",
    "        return loss\n",
    "\n",
    "    def compute_gradient(self, x, y):\n",
    "        \"\"\"\n",
    "\tx : numpy array of shape (minibatch size, 401)\n",
    "\ty : numpy array of shape (minibatch size, 10)\n",
    "\treturns : numpy array of shape (401, 10)\n",
    "\t\"\"\"\n",
    "        scores = x.dot(self.w)\n",
    "        active = (np.multiply(scores, y) < 1).astype(float)\n",
    "        grad = np.dot(-x.T, np.multiply(y, active))\n",
    "        grad = 2 * self.C * grad / y.shape[0]\n",
    "        # Regularization term\n",
    "        grad += self.w\n",
    "        return grad\n",
    "\n",
    "    # Batcher function\n",
    "    def minibatch(self, iterable1, iterable2, size=1):\n",
    "        l = len(iterable1)\n",
    "        n = size\n",
    "        for ndx in range(0, l, n):\n",
    "            index2 = min(ndx + n, l)\n",
    "            yield iterable1[ndx: index2], iterable2[ndx: index2]\n",
    "\n",
    "    def infer(self, x):\n",
    "        \"\"\"\n",
    "\tx : numpy array of shape (number of examples to infer, 401)\n",
    "\treturns : numpy array of shape (number of examples to infer, 10)\n",
    "\t\"\"\"\n",
    "        y = x.dot(self.w)\n",
    "        y_ova = -1 * np.ones(y.shape)\n",
    "        y_ova[np.arange(y_ova.shape[0]), np.argmax(y, axis=1)] = 1\n",
    "        return y_ova\n",
    "\n",
    "    def compute_accuracy(self, y_inferred, y):\n",
    "        y_ova[:, class_index] = 1\n",
    "        return y_ova\n",
    "\n",
    "    def compute_accuracy(self, y_inferred, y):\n",
    "        \"\"\"\n",
    "\ty_inferred : numpy array of shape (number of examples, 10)\n",
    "\ty : numpy array of shape (number of examples, 10)\n",
    "\treturns : float\n",
    "\t\"\"\"\n",
    "        return np.sum(np.all(y == y_inferred, axis=1).astype(float)) / y.shape[0]\n",
    "\n",
    "    def fit(self, x_train, y_train, x_test, y_test):\n",
    "        \"\"\"\n",
    "        x_train : numpy array of shape (number of training examples, 401)\n",
    "        y_train : numpy array of shape (number of training examples, 10)\n",
    "        x_test : numpy array of shape (number of training examples, 401)\n",
    "        y_test : numpy array of shape (number of training examples, 10)\n",
    "        returns : float, float, float, float\n",
    "        \"\"\"\n",
    "        self.num_features = x_train.shape[1]\n",
    "        self.m = y_train.max() + 1\n",
    "        y_train = self.make_one_versus_all_labels(y_train, self.m)\n",
    "        y_test = self.make_one_versus_all_labels(y_test, self.m)\n",
    "        self.w = np.zeros([self.num_features, self.m])\n",
    "        \n",
    "        train_loss_epoch = list()\n",
    "        train_acc_epoch = list()\n",
    "        test_loss_epoch = list()\n",
    "        test_acc_epoch = list()\n",
    "        iterations = list()\n",
    "        \n",
    "        for iteration in range(self.niter):\n",
    "            # Train one pass through the training set\n",
    "            for x, y in self.minibatch(x_train, y_train, size=self.batch_size):\n",
    "                grad = self.compute_gradient(x, y)\n",
    "                self.w -= self.eta * grad\n",
    "\n",
    "            # Measure loss and accuracy on training set\n",
    "            train_loss = self.compute_loss(x_train, y_train)\n",
    "            y_inferred = self.infer(x_train)\n",
    "            train_accuracy = self.compute_accuracy(y_inferred, y_train)\n",
    "\n",
    "            # Measure loss and accuracy on test set\n",
    "            test_loss = self.compute_loss(x_test, y_test)\n",
    "            y_inferred = self.infer(x_test)\n",
    "            test_accuracy = self.compute_accuracy(y_inferred, y_test)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"Iteration %d:\" % iteration)\n",
    "                print(\"Train accuracy: %f\" % train_accuracy)\n",
    "                print(\"Train loss: %f\" % train_loss)\n",
    "                print(\"Test accuracy: %f\" % test_accuracy)\n",
    "                print(\"Test loss: %f\" % test_loss)\n",
    "                print(\"\")\n",
    "            \n",
    "            train_loss_epoch.append(train_loss)\n",
    "            train_acc_epoch.append(train_accuracy)\n",
    "            test_loss_epoch.append(test_loss)\n",
    "            test_acc_epoch.append(test_accuracy)\n",
    "            iterations.append(iteration)\n",
    "            \n",
    "        return train_loss_epoch, train_acc_epoch, test_loss_epoch, test_acc_epoch, iterations\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the data files\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/My Drive/ift_6390/homework2/\n",
    "    x_train = np.load(\"train_features.npy\")\n",
    "    x_test = np.load(\"test_features.npy\")\n",
    "    y_train = np.load(\"train_labels.npy\")\n",
    "    y_test = np.load(\"test_labels.npy\")\n",
    "             \n",
    "    def plot(loss_or_acc,iterations, title, ylabel, is_loss):\n",
    "      plt.plot(iterations,loss_or_acc[0],label=\"C = 0.1\")\n",
    "      plt.plot(iterations,loss_or_acc[1],label=\"C = 1\")\n",
    "      plt.plot(iterations,loss_or_acc[2],label=\"C = 30\")\n",
    "      plt.plot(iterations,loss_or_acc[3],label=\"C = 50\")\n",
    "      plt.legend()\n",
    "      plt.title(title)\n",
    "      plt.xlabel('Number of Epoch')\n",
    "      plt.ylabel(ylabel)\n",
    "      if is_loss:\n",
    "        plt.yscale(\"log\")\n",
    "      plt.show()\n",
    "        #plt.savefig(title+'.png', bbox_inches='tight')\n",
    "      \n",
    "    \n",
    "    C = [0.1, 1, 30, 50]\n",
    "    \n",
    "    EPOCH = 200\n",
    "    \n",
    "    train_losses = list()\n",
    "    train_acc = list()\n",
    "    test_losses = list()\n",
    "    test_acc = list()\n",
    "    \n",
    "    for c in C:\n",
    "        svm = SVM(eta=0.001, C=c, niter=EPOCH, batch_size=5000, verbose=False)\n",
    "        # to compute the gradient or loss before training, do the following:\n",
    "        y_train_ova = svm.make_one_versus_all_labels(y_train, 10) # one-versus-all labels\n",
    "        svm.w = np.zeros([401, 10])\n",
    "        grad = svm.compute_gradient(x_train, y_train_ova)\n",
    "        loss = svm.compute_loss(x_train, y_train_ova)\n",
    "\n",
    "        train_loss_epoch, train_acc_epoch, test_loss_epoch, test_acc_epoch, iterations = svm.fit(x_train, y_train, x_test, y_test)\n",
    "        \n",
    "        train_losses.append(train_loss_epoch)\n",
    "        train_acc.append(train_acc_epoch)\n",
    "        test_losses.append(test_loss_epoch)\n",
    "        test_acc.append(test_acc_epoch)\n",
    "    \n",
    "    title_train_loss = \"Training Loss per Epoch\"\n",
    "    ylabel_train_loss = 'Training Loss'\n",
    "    title_train_acc = \"Training Accuracy per Epoch\"\n",
    "    ylabel_train_acc = 'Training Accuracy'\n",
    "    title_test_loss = \"Testing Loss per Epoch\"\n",
    "    ylabel_test_loss = 'Testing Loss'\n",
    "    title_test_acc = \"Testing Accuracy per Epoch\"\n",
    "    ylabel_test_acc = 'Testing Accuracy'\n",
    "\n",
    "    plot(train_losses,iterations,title_train_loss,ylabel_train_loss, is_loss=True)\n",
    "    plot(train_acc,iterations,title_train_acc,ylabel_train_acc, is_loss=False)\n",
    "    plot(test_losses,iterations,title_test_loss,ylabel_test_loss, is_loss=True)\n",
    "    plot(test_acc,iterations,title_test_acc,ylabel_test_acc, is_loss=False)\n",
    "    \n",
    "\n",
    "    # to infer after training, do the following:\n",
    "    #y_inferred = svm.infer(x_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "kLb-7uyILKL4",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "colab": {
   "name": "Question_4_Plotting.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "TPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
